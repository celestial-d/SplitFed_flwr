{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable,Dict,Optional,Tuple\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('model'))))\n",
    "from model import *\n",
    "import pickle\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_client():\n",
    "  #data loader\n",
    "  '''Load data from source'''\n",
    "  transform = transforms.Compose(\n",
    "  [transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,))]\n",
    "  )\n",
    "  trainset = torchvision.datasets.MNIST(root='./dataset',train=True,download=True,transform=transform)\n",
    "  print(trainset)\n",
    "  trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "  print(len(trainloader))\n",
    "  print(len(trainloader))\n",
    "  return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./dataset\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "1875\n",
      "1875\n"
     ]
    }
   ],
   "source": [
    "server_net = Server_Net()\n",
    "client_net = Client_Net()\n",
    "train_loader = load_data_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1875*32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## client part 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_client(trainloader, epochs):\n",
    "  client_net.eval()\n",
    "  for i,data in enumerate(trainloader):\n",
    "      images, labels = data\n",
    "      output= client_net(images)\n",
    "      output = output.data\n",
    "      output_arr = output.numpy()\n",
    "      target_arr = labels.numpy()\n",
    "      if i==0:\n",
    "        print(output_arr.shape)\n",
    "        print(target_arr.shape)\n",
    "        input_arr = output_arr\n",
    "        label_arr = target_arr\n",
    "      else:\n",
    "        input_arr = np.concatenate((input_arr,output_arr),axis=0)\n",
    "        label_arr = np.concatenate((label_arr,target_arr),axis=0)\n",
    "  return input_arr, label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16, 5, 5)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "input,label = train_client(train_loader,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## server part 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_server():\n",
    "    #data loader\n",
    "    '''load data from server'''\n",
    "    #load code, set loader by transformed data\n",
    "    loaders = []\n",
    "    for clientsID in range(1):\n",
    "        transformed_input_tensor = torch.Tensor(input)\n",
    "        transformed_label_tensor = torch.Tensor(label).type(torch.LongTensor)\n",
    "\n",
    "        transformed_data = TensorDataset(transformed_input_tensor,transformed_label_tensor)\n",
    "        transformed_loader = DataLoader(transformed_data,batch_size=32,shuffle=False)\n",
    "    loaders.append(transformed_loader)\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitParameter(net):\n",
    "  client_layer = {}\n",
    "  num_client_layer=2\n",
    "  for index,(name, val) in enumerate(net.state_dict().items()):\n",
    "    if index >= num_client_layer*2:\n",
    "      break\n",
    "    client_layer[name] = val\n",
    "  return client_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_server(net, loaders):\n",
    "  \"\"\"Train the frozen network on the training set, only one time\"\"\"\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.SGD(net.parameters(),lr = 0.001,momentum=0.9)\n",
    "  net.train() # frozen part이기 때문에 eval mode에서 진행\n",
    "  num_examples = 0\n",
    "  for trainloader in loaders:\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(net(images), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_examples += labels.size(0)\n",
    "  #weight에서 client부분 분리하여 전달\n",
    "  net = splitParameter(net)\n",
    "  print(net.keys())\n",
    "  return [val.cpu().numpy() for _, val in net.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias'])\n"
     ]
    }
   ],
   "source": [
    "server_loaders = load_data_server()\n",
    "parameter = train_server(server_net,server_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 3, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_net.state_dict()['conv1.weight'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([val.cpu().numpy() for _, val in client_net.state_dict().items()])\n",
    "len([val.cpu().numpy() for _, val in client_net.state_dict().items()][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정확도 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parameter.pkl','rb') as f:\n",
    "    test_net = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test():\n",
    "  #data loader\n",
    "  '''Load data from source'''\n",
    "  transform = transforms.Compose(\n",
    "  [transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,))]\n",
    "  )\n",
    "  testset = torchvision.datasets.MNIST(root='./dataset',train=False,download=True,transform=transform)\n",
    "  testloader = DataLoader(testset, batch_size=32)\n",
    "  return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net,test_loader):\n",
    "    net.eval()\n",
    "    correct=0\n",
    "    total=0\n",
    "    for images, labels in test_loader:\n",
    "        _,predicted = torch.max(net.test(images).data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "    print(100*correct/total)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = load_data_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.02\n"
     ]
    }
   ],
   "source": [
    "eval(test_net,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('jh': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cdeab2c947337ce8e9bd7d550378ccb0db9e342b3e889fad1a3f0ac974c3d8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
